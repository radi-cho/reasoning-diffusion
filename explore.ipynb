{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419db978-5d94-44c2-ac52-61516d1307db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776c3e2f9ff14fb498efd75a93021c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LLaDAModelLM were not initialized from the model checkpoint at ./LLaDA-8B-Instruct and are newly initialized: ['model.transformer.mask_head.bias', 'model.transformer.mask_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "device = 'cuda'\n",
    "model = AutoModel.from_pretrained('./LLaDA-8B-Instruct', trust_remote_code=True, torch_dtype=torch.bfloat16, use_cache=False).to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('./LLaDA-8B-Instruct', trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a0762f-a98e-4bf6-b9e0-3dc148125799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0214, -0.0079,  0.0208,  ..., -0.0176,  0.0060, -0.0128]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "mask_head = model.model.transformer.mask_head\n",
    "torch.nn.init.kaiming_uniform_(mask_head.weight, nonlinearity='linear')\n",
    "if mask_head.bias is not None:\n",
    "    torch.nn.init.zeros_(mask_head.bias)\n",
    "print(mask_head.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd05b52-c4c6-4018-a253-dcbb3b7b85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e6114-0508-46fb-995d-c84511228d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_length = 128\n",
    "steps = 128\n",
    "block_length = 32\n",
    "print('*' * 66)\n",
    "print(f'**  Answer Length: {gen_length}  |  Sampling Steps: {steps}  **')\n",
    "print('*' * 66)\n",
    "\n",
    "conversation_num = 0\n",
    "user_input = \"How much is 2+2?\"\n",
    "\n",
    "m = [{\"role\": \"user\", \"content\": user_input}]\n",
    "user_input = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "input_ids = tokenizer(user_input)['input_ids']\n",
    "input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "\n",
    "if conversation_num == 0:\n",
    "    prompt = input_ids\n",
    "else:\n",
    "    prompt = torch.cat([prompt, input_ids[:, 1:]], dim=1)\n",
    "\n",
    "out = model.generate(prompt, steps=steps, gen_length=gen_length, block_length=block_length, temperature=0., cfg_scale=0., remasking='random')\n",
    "\n",
    "answer = tokenizer.batch_decode(out[:, prompt.shape[1]:], skip_special_tokens=True)[0]\n",
    "print(f\"Bot's reply: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16823450-a9a5-4e58-a331-f3eec714d746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28532eb7-3a14-4a2b-ad28-fbb524860ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9db66c07-bdbe-4b31-bfa7-a27a2a82eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "207358fc-421c-48bb-8dfb-f40ae8e352d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\")\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            #{'role': 'user', 'content': 'What is the largest single-digit prime number?'},\n",
    "            #{'role': 'assistant', 'content': XML_COT_FORMAT.format(\n",
    "            #    reasoning=\"9 is divisble by 3 and 8 is divisible by 2, but 7 is prime.\",\n",
    "            #    answer=\"7\"\n",
    "            #)},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f8b198a-c3d8-4109-bec7-fbf99dc7cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses] \n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses] \n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49823aa7-b881-4991-83b8-8662fd85237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"outputs/LLaDA-GRPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0956894c-7733-41e6-abef-0ceb0dbebe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5770485-2b21-4c54-9c6c-24385eca875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    # run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=786,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    log_on_each_node=False,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6095ce-cf1a-400e-ad24-091382de8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    # peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e1487-3004-4e04-a63c-9d5e5f0e7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2748e73d-4411-40d9-938a-7e6b04f00ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftext|>user\\n\\nsay hiassistant\\n\\nHello! How can I assist you today?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([126080, 126346,   3840, 126347,    198,    198,  48851,  26612, 126348, 126346,    598,  10450, 126347,    198,    198,  14455,      0,   2071,560,    331,   6528,    362,   3342,     30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7886402-f9ee-4b85-a6f9-fa8b565cfb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: UNDERSTAND TOKEN STRUCTURE\n",
    "# tensor([[126080, 126346,   3840, 126347,    198,    198,  48851,  26612, 126348,\n",
    "#          126346,    598,  10450, 126347,    198,    198,  14455,      0,   2071,\n",
    "#             560,    331,   6528,    362,   3342,     30, 126348, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
    "#          126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081]],\n",
    "#        device='cuda:0')\n",
    "# Bot's reply: Hello! How can I assist you today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
