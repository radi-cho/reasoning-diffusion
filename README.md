# LLaDA-R1: Scaling Reasoning at Inference Time with Diffusion-LLMs
## Etched, Mercor, Cognition ITC Hackathon 2025

We present LLaDA-R1 (LLaDA Reasoner1), the first text diffusion model optimized for reasoning and computational efficiency at inference time. We present three key innovations: (1) a remasking optimization head that learns optimal token positions to remask at each step, (2) reinforcement learning with a dual-objective reward function balancing reasoning accuracy and computational efficiency plus a supervised fine tuning approach and (3) an adaptive diffusion method that dynamically adjusts diffusion steps based on confidence convergence. We challenge the assumption that autoregressive architectures are inherently superior for reasoning tasks and highlight the untapped potential of properly optimized diffusion-based approaches for complex problem-solving. 

A preprint technical report of our work can be found below: https://docs.google.com/document/d/1fEqNj_o1NQ1_Cyc55KtH54z0jaOSyPY4agF8KRaLgqo/edit?usp=sharing.
