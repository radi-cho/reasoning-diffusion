{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f0f0022b3c14b379d3ae51174e37ad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7258984b2ee54f8bbcbb4590b4b44dc0",
              "IPY_MODEL_411a029f6b7343e38735bcdb27443d85",
              "IPY_MODEL_dc9041880dd44451acd9964ca85dbcd8"
            ],
            "layout": "IPY_MODEL_b747f6627375479d8e755e370db602ad"
          }
        },
        "7258984b2ee54f8bbcbb4590b4b44dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_372447f6ff22495883155129fa51aa21",
            "placeholder": "​",
            "style": "IPY_MODEL_81f7e8c134b740b9985091b0d7cacd52",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "411a029f6b7343e38735bcdb27443d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d42ed4f5fd04dba832a027f6c701d3f",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a44424b8cc294ae29a481522a40874bd",
            "value": 6
          }
        },
        "dc9041880dd44451acd9964ca85dbcd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aa218e3603f4dd0b6a9d8b2833f646f",
            "placeholder": "​",
            "style": "IPY_MODEL_a5953f78d15d44b0a873e220d96ed282",
            "value": " 6/6 [00:01&lt;00:00,  3.30it/s]"
          }
        },
        "b747f6627375479d8e755e370db602ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372447f6ff22495883155129fa51aa21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81f7e8c134b740b9985091b0d7cacd52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d42ed4f5fd04dba832a027f6c701d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44424b8cc294ae29a481522a40874bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1aa218e3603f4dd0b6a9d8b2833f646f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5953f78d15d44b0a873e220d96ed282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VErp3AUvJd_E"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from modeling_llada import LLaDAConfig, ModuleType, init_weights\n",
        "from collections import defaultdict\n",
        "\n",
        "rom peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "import subprocess\n",
        "subprocess.check_call([\"pip\", \"install\", \"peft\"])\n",
        "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def print_gpu_memory():\n",
        "    \"\"\"Print current GPU memory usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            total_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
        "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
        "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
        "            free = total_mem - reserved\n",
        "            print(f\"GPU {i}: Total {total_mem:.2f} GB | Reserved {reserved:.2f} GB | Allocated {allocated:.2f} GB | Free {free:.2f} GB\")\n",
        "\n",
        "MAX_LENGTH = 384\n",
        "BATCH_SIZE = 1\n",
        "LEARNING_RATE = 1e-5\n",
        "MASK_HEAD_LR = 5e-5\n",
        "BASE_LR = 1e-6\n",
        "NUM_EPOCHS = 2\n",
        "N_SHOT = 4\n",
        "COT_FLAG = True\n",
        "ANSWER_TRIGGER = \"The answer is\"\n",
        "GEN_LENGTH = 64\n",
        "BLOCK_LENGTH = 8\n",
        "\n",
        "ANS_RE = re.compile(r\"#### (\\-?[0-9\\.\\,]+)\")\n",
        "INVALID_ANS = \"[invalid]\"\n",
        "\n",
        "def extract_answer_from_output(completion):\n",
        "    \"\"\"Extract answer using GSM8K format with #### marker\"\"\"\n",
        "    match = ANS_RE.search(completion)\n",
        "    if match:\n",
        "        match_str = match.group(1).strip()\n",
        "        match_str = match_str.replace(\",\", \"\")\n",
        "        return match_str\n",
        "    else:\n",
        "        return INVALID_ANS\n",
        "\n",
        "def is_correct(model_answer, answer):\n",
        "    \"\"\"Check if model's answer matches ground truth\"\"\"\n",
        "    gt_answer = extract_answer_from_output(answer)\n",
        "    assert gt_answer != INVALID_ANS\n",
        "    return model_answer == gt_answer\n",
        "\n",
        "def clean_answer(model_pred):\n",
        "    \"\"\"Clean and extract numerical answer from model prediction\"\"\"\n",
        "    model_pred = model_pred.lower()\n",
        "    preds = model_pred.split(ANSWER_TRIGGER.lower())\n",
        "    answer_flag = True if len(preds) > 1 else False\n",
        "    if answer_flag:\n",
        "        # Pick first answer with flag\n",
        "        pred = preds[1]\n",
        "    else:\n",
        "        # Pick last number without flag\n",
        "        pred = preds[-1]\n",
        "\n",
        "    pred = pred.replace(\",\", \"\")\n",
        "    pred = [s for s in re.findall(r\"-?\\d+\\.?\\d*\", pred)]\n",
        "\n",
        "    if len(pred) == 0:\n",
        "        return INVALID_ANS\n",
        "\n",
        "    if answer_flag:\n",
        "        # choose the first element in list\n",
        "        pred = pred[0]\n",
        "    else:\n",
        "        # choose the last element in list\n",
        "        pred = pred[-1]\n",
        "\n",
        "    # (For arithmetic tasks) if a word ends with period, it will be omitted\n",
        "    if pred[-1] == \".\":\n",
        "        pred = pred[:-1]\n",
        "\n",
        "    return pred\n",
        "\n",
        "def create_demo_text(n_shot=4, cot_flag=True):\n",
        "    \"\"\"Create demonstration examples for few-shot learning with reduced examples\"\"\"\n",
        "    question, chain, answer = [], [], []\n",
        "    question.append(\n",
        "        \"There are 15 trees in the grove. \"\n",
        "        \"Grove workers will plant trees in the grove today. \"\n",
        "        \"After they are done, there will be 21 trees. \"\n",
        "        \"How many trees did the grove workers plant today?\"\n",
        "    )\n",
        "    chain.append(\n",
        "        \"There are 15 trees originally. \"\n",
        "        \"Then there were 21 trees after some more were planted. \"\n",
        "        \"So there must have been 21 - 15 = 6.\"\n",
        "    )\n",
        "    answer.append(\"6\")\n",
        "\n",
        "    question.append(\n",
        "        \"If there are 3 cars in the parking lot and 2 more cars arrive, \"\n",
        "        \"how many cars are in the parking lot?\"\n",
        "    )\n",
        "    chain.append(\"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.\")\n",
        "    answer.append(\"5\")\n",
        "\n",
        "    question.append(\n",
        "        \"Leah had 32 chocolates and her sister had 42. If they ate 35, \"\n",
        "        \"how many pieces do they have left in total?\"\n",
        "    )\n",
        "    chain.append(\n",
        "        \"Originally, Leah had 32 chocolates. \"\n",
        "        \"Her sister had 42. So in total they had 32 + 42 = 74. \"\n",
        "        \"After eating 35, they had 74 - 35 = 39.\"\n",
        "    )\n",
        "    answer.append(\"39\")\n",
        "\n",
        "    question.append(\n",
        "        \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason \"\n",
        "        \"has 12 lollipops. How many lollipops did Jason give to Denny?\"\n",
        "    )\n",
        "    chain.append(\n",
        "        \"Jason started with 20 lollipops. Then he had 12 after giving some \"\n",
        "        \"to Denny. So he gave Denny 20 - 12 = 8.\"\n",
        "    )\n",
        "    answer.append(\"8\")\n",
        "\n",
        "    # Randomize order of the examples\n",
        "    index_list = list(range(len(question)))\n",
        "    random.shuffle(index_list)\n",
        "\n",
        "    # Concatenate demonstration examples\n",
        "    demo_text = \"\"\n",
        "    for i in index_list[:n_shot]:\n",
        "        if cot_flag:\n",
        "            demo_text += (\n",
        "                \"Q: \"\n",
        "                + question[i]\n",
        "                + \"\\nA: \"\n",
        "                + chain[i]\n",
        "                + \" \"\n",
        "                + ANSWER_TRIGGER\n",
        "                + \" \"\n",
        "                + answer[i]\n",
        "                + \".\\n\\n\"\n",
        "            )\n",
        "        else:\n",
        "            demo_text += (\n",
        "                \"Question: \"\n",
        "                + question[i]\n",
        "                + \"\\nAnswer: \"\n",
        "                + ANSWER_TRIGGER\n",
        "                + \" \"\n",
        "                + answer[i]\n",
        "                + \".\\n\\n\"\n",
        "            )\n",
        "    return demo_text\n",
        "\n",
        "def build_prompt(input_text, n_shot, cot_flag):\n",
        "    \"\"\"Build the full prompt with demonstrations and query\"\"\"\n",
        "    demo = create_demo_text(n_shot, cot_flag)\n",
        "    input_text_prompt = demo + \"Q: \" + input_text + \"\\n\" + \"A:\"\n",
        "    return input_text_prompt\n",
        "\n",
        "class GSM8KDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length=MAX_LENGTH, n_shot=N_SHOT, cot_flag=COT_FLAG, subset_size=100):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.n_shot = n_shot\n",
        "        self.cot_flag = cot_flag\n",
        "\n",
        "        self.questions = []\n",
        "        self.answers = []\n",
        "        self.extracted_answers = []\n",
        "\n",
        "        # Use a subset of the dataset for memory efficiency\n",
        "        subset = dataset.select(range(min(subset_size, len(dataset))))\n",
        "\n",
        "        # Process each example\n",
        "        for example in subset:\n",
        "            question = example[\"question\"]\n",
        "            answer_with_solution = example[\"answer\"]\n",
        "\n",
        "            # Extract the numerical answer from the solution\n",
        "            numerical_answer = extract_answer_from_output(answer_with_solution)\n",
        "\n",
        "            if numerical_answer != INVALID_ANS:\n",
        "                self.questions.append(question)\n",
        "                self.answers.append(answer_with_solution)\n",
        "                self.extracted_answers.append(numerical_answer)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        answer = self.answers[idx]\n",
        "        extracted_answer = self.extracted_answers[idx]\n",
        "\n",
        "        # Format the prompt using GSM8K prompt builder\n",
        "        prompt = build_prompt(question, self.n_shot, self.cot_flag)\n",
        "\n",
        "        # Tokenize\n",
        "        tokenized_prompt = self.tokenizer(prompt, return_tensors=\"pt\",\n",
        "                                         padding=\"max_length\",\n",
        "                                         truncation=True,\n",
        "                                         max_length=self.max_length)\n",
        "\n",
        "        # Don't move to device here - we'll do that in the training loop\n",
        "        # This keeps CPU memory usage lower\n",
        "        return {\n",
        "            \"input_ids\": tokenized_prompt[\"input_ids\"][0],\n",
        "            \"attention_mask\": tokenized_prompt[\"attention_mask\"][0],\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"extracted_answer\": extracted_answer\n",
        "        }\n",
        "\n",
        "# Add Gumbel noise for the diffusion process\n",
        "def add_gumbel_noise(logits, temperature=1.0):\n",
        "    \"\"\"Add Gumbel noise to logits for sampling.\"\"\"\n",
        "    if temperature == 0:\n",
        "        return logits\n",
        "    gumbel = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)\n",
        "    return logits + gumbel * temperature\n",
        "\n",
        "# Memory-efficient generation with step counting\n",
        "def generate_with_step_count(model, prompt, max_steps=32, gen_length=64, block_length=8,\n",
        "                           temperature=0.7, remasking='mask_predictor',\n",
        "                           mask_id=126336, convergence_threshold=0.01, patience=3):\n",
        "    '''Memory-efficient version of the generate function with step counting'''\n",
        "    # Ensure CUDA device\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    if not prompt.is_cuda:\n",
        "        prompt = prompt.to(device)\n",
        "\n",
        "    prompt_length = prompt.shape[1]\n",
        "\n",
        "    # Use a smaller generation length to reduce memory usage\n",
        "    x = torch.full((1, prompt_length + gen_length), mask_id, dtype=torch.long, device=device)\n",
        "    x[:, :prompt_length] = prompt.clone()\n",
        "\n",
        "    prompt_index = (x != mask_id)\n",
        "\n",
        "    assert gen_length % block_length == 0\n",
        "    num_blocks = gen_length // block_length\n",
        "\n",
        "    total_steps_used = 0\n",
        "\n",
        "    for num_block in range(num_blocks):\n",
        "        # Clear memory between blocks\n",
        "        if num_block > 0:\n",
        "            clear_memory()\n",
        "\n",
        "        # Get mask index for current block\n",
        "        block_start = prompt_length + num_block * block_length\n",
        "        block_end = prompt_length + (num_block + 1) * block_length\n",
        "\n",
        "        # Continue until convergence or max_steps\n",
        "        for step in range(max_steps):\n",
        "            mask_index = (x == mask_id)\n",
        "\n",
        "            # Skip if no more masks in this block\n",
        "            current_block_mask_indices = mask_index[:, block_start:block_end]\n",
        "            num_masked = current_block_mask_indices.sum().item()\n",
        "\n",
        "            if num_masked == 0:\n",
        "                break\n",
        "\n",
        "            total_steps_used += 1\n",
        "\n",
        "            # Get the model outputs - using half precision to save memory\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(x)\n",
        "                logits = outputs.logits\n",
        "                mask_logits = outputs.mask_logits if hasattr(outputs, 'mask_logits') else None\n",
        "\n",
        "            # Sample next tokens\n",
        "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
        "            x0 = torch.argmax(logits_with_noise, dim=-1)  # b, l\n",
        "\n",
        "            # Calculate confidence scores\n",
        "            if remasking == 'mask_predictor' and mask_logits is not None:\n",
        "                # Use mask predictor scores directly\n",
        "                confidence = torch.sigmoid(mask_logits.squeeze(-1))  # Convert to probabilities\n",
        "            elif remasking == 'low_confidence':\n",
        "                p = F.softmax(logits, dim=-1)\n",
        "                x0_p = torch.squeeze(\n",
        "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1)\n",
        "                confidence = 1.0 - x0_p  # Invert so higher means less confident\n",
        "            elif remasking == 'random':\n",
        "                confidence = torch.rand((x0.shape[0], x0.shape[1]), device=device)\n",
        "            else:\n",
        "                raise NotImplementedError(remasking)\n",
        "\n",
        "            # Don't consider tokens outside the current block\n",
        "            confidence[:, :block_start] = float('-inf')  # Before current block\n",
        "            confidence[:, block_end:] = float('-inf')    # After current block\n",
        "\n",
        "            # Update tokens\n",
        "            x0 = torch.where(mask_index, x0, x)\n",
        "\n",
        "            # Determine how many tokens to replace\n",
        "            # Adaptive schedule: later steps unmask fewer tokens\n",
        "            remaining_ratio = 1.0 - (step / max_steps)\n",
        "            num_to_unmask = max(1, int(num_masked * remaining_ratio * 0.5))  # Unmask fewer tokens at once\n",
        "\n",
        "            # Only consider remasking tokens in the current block that were previously masked\n",
        "            remask_candidates = mask_index.clone()\n",
        "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=device)\n",
        "\n",
        "            for j in range(confidence.shape[0]):\n",
        "                # Only select from tokens that were previously masked\n",
        "                masked_confidence = confidence[j].clone()\n",
        "                masked_confidence[~remask_candidates[j]] = float('-inf')\n",
        "\n",
        "                # Select the tokens with lowest confidence to unmask\n",
        "                _, select_index = torch.topk(masked_confidence,\n",
        "                                          k=min(num_to_unmask, torch.sum(remask_candidates[j]).item()),\n",
        "                                          largest=False)\n",
        "                transfer_index[j, select_index] = True\n",
        "\n",
        "            # Update the tokens\n",
        "            x[transfer_index] = x0[transfer_index]\n",
        "\n",
        "            # Clear some memory\n",
        "            del logits, mask_logits, x0, confidence, masked_confidence\n",
        "            clear_memory()\n",
        "\n",
        "            # If all tokens in the block are unmasked, move to the next block\n",
        "            if not (x[:, block_start:block_end] == mask_id).any():\n",
        "                break\n",
        "\n",
        "        # If we reached max_steps, finalize the block by unmasking any remaining tokens\n",
        "        remaining_mask_indices = (x[:, block_start:block_end] == mask_id)\n",
        "        if remaining_mask_indices.any():\n",
        "            total_steps_used += 1\n",
        "            # Get predictions for remaining masked tokens\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(x)\n",
        "                logits = outputs.logits\n",
        "                logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
        "                x0 = torch.argmax(logits_with_noise, dim=-1)\n",
        "\n",
        "            # Unmask all remaining tokens in this block\n",
        "            mask_indices_full = (x == mask_id)\n",
        "            block_mask_indices = torch.zeros_like(mask_indices_full, device=device)\n",
        "            block_mask_indices[:, block_start:block_end] = remaining_mask_indices\n",
        "\n",
        "            # Replace only the masked tokens in the current block\n",
        "            x = torch.where(block_mask_indices, x0, x)\n",
        "\n",
        "            # Clear memory\n",
        "            del logits, x0, mask_indices_full, block_mask_indices\n",
        "            clear_memory()\n",
        "\n",
        "    return x, total_steps_used\n",
        "\n",
        "# Custom reward calculation\n",
        "def calculate_reward(generated_text, target_answer, num_steps_used, max_steps):\n",
        "    \"\"\"\n",
        "    Calculate reward based on correctness and efficiency of diffusion.\n",
        "\n",
        "    Args:\n",
        "        generated_text: Text generated by the model\n",
        "        target_answer: The correct numerical answer\n",
        "        num_steps_used: Number of diffusion steps used\n",
        "        max_steps: Maximum allowed diffusion steps\n",
        "\n",
        "    Returns:\n",
        "        float: Calculated reward\n",
        "    \"\"\"\n",
        "    # Extract the model's answer using GSM8K extraction\n",
        "    predicted_answer = extract_answer_from_output(generated_text)\n",
        "\n",
        "    if predicted_answer == INVALID_ANS:\n",
        "        return -5.0  # Penalty for no clear answer\n",
        "\n",
        "    # Base reward for correctness (high reward for correct answer)\n",
        "    correctness_reward = 10.0 if predicted_answer == target_answer else -5.0\n",
        "\n",
        "    # Efficiency reward (penalize using too many steps)\n",
        "    efficiency_factor = max(0, 2.0 - 2.0 * (num_steps_used / max_steps))\n",
        "    efficiency_reward = 5.0 * efficiency_factor\n",
        "\n",
        "    total_reward = correctness_reward + efficiency_reward\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "# Apply LoRA to a model for parameter-efficient fine-tuning\n",
        "def apply_lora_to_model(model, device):\n",
        "    \"\"\"\n",
        "    Apply LoRA (Low-Rank Adaptation) to a model for parameter-efficient fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        model: The base model to apply LoRA to\n",
        "        device: The device to put the model on\n",
        "\n",
        "    Returns:\n",
        "        The model with LoRA adapters\n",
        "    \"\"\"\n",
        "    print(\"Applying LoRA for parameter-efficient tuning...\")\n",
        "\n",
        "    # Define target modules based on model architecture\n",
        "    # For LLaMA-like models, target the attention projection layers\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ]\n",
        "\n",
        "    # Prepare model for LoRA if 4/8bit quantization is used\n",
        "    # model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Configure LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=16,                # Rank of the update matrices\n",
        "        lora_alpha=32,       # Scaling factor for the learned weights\n",
        "        lora_dropout=0.05,   # Dropout probability for LoRA layers\n",
        "        bias=\"none\",         # Don't train bias parameters\n",
        "        target_modules=target_modules,\n",
        "        modules_to_save=[\"mask_predictor\"]  # Save non-LoRA modules\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Move the model to the device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print trainable parameters for verification\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Custom output class for LLaDA\n",
        "class UpdatedLLaDAOutput:\n",
        "    def __init__(self, logits, attn_key_values=None, hidden_states=None, mask_logits=None):\n",
        "        self.logits = logits\n",
        "        self.attn_key_values = attn_key_values\n",
        "        self.hidden_states = hidden_states\n",
        "        self.mask_logits = mask_logits\n",
        "\n",
        "# Add mask predictor to model and modify forward methods\n",
        "def prepare_model_with_mask_predictor(base_model, device):\n",
        "    \"\"\"Add mask predictor to model and patch forward methods to handle mask prediction\"\"\"\n",
        "\n",
        "    # Add the mask prediction head to the model\n",
        "    d_model = base_model.config.d_model\n",
        "    include_bias = base_model.config.include_bias\n",
        "\n",
        "    base_model.model.mask_predictor = torch.nn.Linear(\n",
        "        d_model,\n",
        "        1,\n",
        "        bias=include_bias,\n",
        "    ).to(device).to(torch.bfloat16)\n",
        "\n",
        "    print(\"Added mask predictor to CUDA\")\n",
        "    print_gpu_memory()\n",
        "\n",
        "    # Initialize the weights of the mask prediction head\n",
        "    init_weights(\n",
        "        base_model.model.config,\n",
        "        base_model.model.mask_predictor,\n",
        "        d=d_model,\n",
        "        type_of_module=ModuleType.out_module\n",
        "    )\n",
        "\n",
        "    # Monkey patch the forward method to include mask_logits in the output\n",
        "    original_forward = base_model.model.forward\n",
        "\n",
        "    def forward_with_mask_logits(*args, **kwargs):\n",
        "        outputs = original_forward(*args, **kwargs)\n",
        "\n",
        "        # Extract the final hidden states to apply the mask predictor\n",
        "        hidden_states = None\n",
        "        if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
        "            hidden_states = outputs.hidden_states[-1]\n",
        "        else:\n",
        "            # This is a fallback for when hidden states aren't available\n",
        "            pass\n",
        "\n",
        "        # Apply the mask predictor if we have hidden states\n",
        "        mask_logits = None\n",
        "        if hidden_states is not None:\n",
        "            mask_logits = base_model.model.mask_predictor(hidden_states)\n",
        "\n",
        "        # Create a new output that includes mask_logits\n",
        "        return UpdatedLLaDAOutput(\n",
        "            logits=outputs.logits,\n",
        "            attn_key_values=getattr(outputs, 'attn_key_values', None),\n",
        "            hidden_states=getattr(outputs, 'hidden_states', None),\n",
        "            mask_logits=mask_logits\n",
        "        )\n",
        "\n",
        "    # Apply the monkey patch\n",
        "    base_model.model.forward = forward_with_mask_logits\n",
        "\n",
        "    # Patch the HuggingFace wrapper's forward method\n",
        "    original_hf_forward = base_model.forward\n",
        "\n",
        "    def hf_forward_with_mask_logits(*args, **kwargs):\n",
        "        # Force output_hidden_states to True to ensure we get the hidden states\n",
        "        kwargs['output_hidden_states'] = True\n",
        "\n",
        "        # Call the original forward method\n",
        "        outputs = original_hf_forward(*args, **kwargs)\n",
        "\n",
        "        # Add the mask_logits attribute to the outputs object if not already there\n",
        "        if not hasattr(outputs, 'mask_logits'):\n",
        "            # Use the hidden states to compute mask_logits\n",
        "            if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
        "                last_hidden_state = outputs.hidden_states[-1]\n",
        "                setattr(outputs, 'mask_logits', base_model.model.mask_predictor(last_hidden_state))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    # Apply the monkey patch\n",
        "    base_model.forward = hf_forward_with_mask_logits\n",
        "\n",
        "    return base_model\n",
        "\n",
        "# Memory-efficient training loop with gradient accumulation for full model RL tuning\n",
        "def train_full_model_with_rl(model, tokenizer, dataset, optimizer, num_epochs=2, max_steps=32,\n",
        "                         accumulation_steps=4, fp16=True):\n",
        "    \"\"\"\n",
        "    Train the full model using RL with memory optimizations and parameter-efficient tuning.\n",
        "\n",
        "    Args:\n",
        "        model: The LLaDA model with LoRA adapters\n",
        "        tokenizer: The tokenizer\n",
        "        dataset: The GSM8K dataset\n",
        "        optimizer: The optimizer\n",
        "        num_epochs: Number of training epochs\n",
        "        max_steps: Maximum number of diffusion steps allowed\n",
        "        accumulation_steps: Number of steps to accumulate gradients\n",
        "        fp16: Whether to use mixed precision training\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    scaler = torch.cuda.amp.GradScaler() if fp16 else None\n",
        "\n",
        "    model.train()\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Statistics tracking\n",
        "    all_rewards = []\n",
        "    all_steps = []\n",
        "    correct_answers = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    # Create log file\n",
        "    with open(\"full_model_training_log.txt\", \"w\") as log_file:\n",
        "        log_file.write(\"Training log for LLaDA RL with full model tuning\\n\\n\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        epoch_rewards = []\n",
        "        epoch_steps = []\n",
        "        epoch_correct = 0\n",
        "        epoch_total = 0\n",
        "\n",
        "        # Reset optimizer at the start of each epoch\n",
        "        optimizer.zero_grad()\n",
        "        update_step = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}\")):\n",
        "            # Move batch to CUDA device explicitly\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            question = batch[\"question\"][0]  # Single item due to batch size 1\n",
        "            target_answer = batch[\"extracted_answer\"][0]  # Single item\n",
        "\n",
        "            # Check that input is on CUDA\n",
        "            if not input_ids.is_cuda:\n",
        "                print(\"WARNING: input_ids not on CUDA! Forcing to CUDA...\")\n",
        "                input_ids = input_ids.to(device)\n",
        "\n",
        "            # Forward pass with step counting - no gradients needed for initial generation\n",
        "            with torch.no_grad():\n",
        "                generated, num_steps = generate_with_step_count(\n",
        "                    model,\n",
        "                    input_ids,\n",
        "                    max_steps=max_steps,\n",
        "                    gen_length=GEN_LENGTH,\n",
        "                    block_length=BLOCK_LENGTH,\n",
        "                    temperature=0.7\n",
        "                )\n",
        "\n",
        "                generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "                # Extract the answer part (after the original prompt)\n",
        "                prompt_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "                answer_part = generated_text[len(prompt_text):]\n",
        "\n",
        "                # Calculate reward\n",
        "                reward = calculate_reward(answer_part, target_answer, num_steps, max_steps)\n",
        "\n",
        "            # Clear memory before optimization step\n",
        "            clear_memory()\n",
        "\n",
        "            # Now run policy-based training with the full model\n",
        "            # We'll use a simplified REINFORCE approach focusing on the next token prediction\n",
        "            with torch.set_grad_enabled(True):\n",
        "                if fp16:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        # Forward pass on a short sequence to get gradients\n",
        "                        # Just use the first 32 tokens for efficiency\n",
        "                        short_input = input_ids[:, :min(32, input_ids.shape[1])]\n",
        "                        outputs = model(short_input)\n",
        "                        logits = outputs.logits\n",
        "\n",
        "                        # Calculate policy gradient loss\n",
        "                        # We'll use a simple proxy loss based on the reward\n",
        "                        # This encourages/discourages the current policy based on the reward\n",
        "                        last_token_logits = logits[:, -1, :]\n",
        "\n",
        "                        # Simple entropy-based loss scaled by reward\n",
        "                        # If reward is positive, we want to reinforce the current policy\n",
        "                        # If reward is negative, we want to explore other options\n",
        "                        policy_loss = -reward * F.log_softmax(last_token_logits, dim=-1).mean()\n",
        "\n",
        "                        # If we have mask logits, add a loss term for those too\n",
        "                        if hasattr(outputs, 'mask_logits') and outputs.mask_logits is not None:\n",
        "                            mask_logits = outputs.mask_logits[:, -1]\n",
        "                            mask_prob = torch.sigmoid(mask_logits)\n",
        "\n",
        "                            # For high rewards (good performance), we want lower mask probabilities (faster unmasking)\n",
        "                            # For low rewards (poor performance), we want higher mask probabilities (more careful unmasking)\n",
        "                            mask_loss = reward * mask_prob.mean()\n",
        "\n",
        "                            # Combine losses, with mask loss weighted lower\n",
        "                            loss = policy_loss + 0.2 * mask_loss\n",
        "                        else:\n",
        "                            loss = policy_loss\n",
        "\n",
        "                        # Scale for gradient accumulation\n",
        "                        loss = loss / accumulation_steps\n",
        "\n",
        "                    # Backward pass with scaling\n",
        "                    scaler.scale(loss).backward()\n",
        "                else:\n",
        "                    # Forward pass on a short sequence to get gradients\n",
        "                    short_input = input_ids[:, :min(32, input_ids.shape[1])]\n",
        "                    outputs = model(short_input)\n",
        "                    logits = outputs.logits\n",
        "\n",
        "                    # Calculate policy gradient loss\n",
        "                    last_token_logits = logits[:, -1, :]\n",
        "                    policy_loss = -reward * F.log_softmax(last_token_logits, dim=-1).mean()\n",
        "\n",
        "                    # Add mask loss if available\n",
        "                    if hasattr(outputs, 'mask_logits') and outputs.mask_logits is not None:\n",
        "                        mask_logits = outputs.mask_logits[:, -1]\n",
        "                        mask_prob = torch.sigmoid(mask_logits)\n",
        "                        mask_loss = reward * mask_prob.mean()\n",
        "                        loss = policy_loss + 0.2 * mask_loss\n",
        "                    else:\n",
        "                        loss = policy_loss\n",
        "\n",
        "                    # Scale for gradient accumulation\n",
        "                    loss = loss / accumulation_steps\n",
        "\n",
        "                    # Backward pass\n",
        "                    loss.backward()\n",
        "\n",
        "            # Update parameters after accumulation_steps\n",
        "            update_step += 1\n",
        "            if update_step % accumulation_steps == 0:\n",
        "                if fp16:\n",
        "                    # Clip gradients to prevent instability\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                    # Step with scaler\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    # Clip gradients to prevent instability\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                    # Step optimizer\n",
        "                    optimizer.step()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                clear_memory()\n",
        "\n",
        "            # Track statistics\n",
        "            all_rewards.append(reward)\n",
        "            all_steps.append(num_steps)\n",
        "            epoch_rewards.append(reward)\n",
        "            epoch_steps.append(num_steps)\n",
        "\n",
        "            # Check if answer is correct\n",
        "            predicted_answer = extract_answer_from_output(answer_part)\n",
        "            is_correct_ans = (predicted_answer == target_answer)\n",
        "\n",
        "            if is_correct_ans:\n",
        "                correct_answers += 1\n",
        "                epoch_correct += 1\n",
        "\n",
        "            total_examples += 1\n",
        "            epoch_total += 1\n",
        "\n",
        "            # Log results\n",
        "            log_message = (\n",
        "                f\"Epoch {epoch+1}, Example {batch_idx+1}: \"\n",
        "                f\"Reward={reward:.2f}, Steps={num_steps}, \"\n",
        "                f\"Correct={is_correct_ans}, \"\n",
        "                f\"Target={target_answer}, Predicted={predicted_answer}\\n\"\n",
        "            )\n",
        "\n",
        "            with open(\"full_model_training_log.txt\", \"a\") as log_file:\n",
        "                log_file.write(log_message)\n",
        "\n",
        "            # Print occasional updates (less frequently to reduce overhead)\n",
        "            if total_examples % 5 == 0:\n",
        "                print(f\"\\nQuestion: {question}\")\n",
        "                print(f\"Generated: {answer_part[:50]}...\")  # Show first 50 chars\n",
        "                print(f\"Correct answer: {target_answer}, Predicted: {predicted_answer}\")\n",
        "                print(f\"Reward: {reward:.2f}, Steps: {num_steps}, Is correct: {is_correct_ans}\")\n",
        "                print(f\"Avg reward so far: {sum(all_rewards)/max(1, len(all_rewards)):.2f}\")\n",
        "                print(f\"Accuracy so far: {correct_answers/max(1, total_examples):.2%}\")\n",
        "                print(f\"Avg steps: {sum(all_steps)/max(1, len(all_steps)):.1f}\")\n",
        "                print_gpu_memory()"
      ],
      "metadata": {
        "id": "B7wgp4rWNAgA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"GSAI-ML/LLaDA-8B-Instruct\")\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"GSAI-ML/LLaDA-8B-Instruct\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map='auto'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "2f0f0022b3c14b379d3ae51174e37ad6",
            "7258984b2ee54f8bbcbb4590b4b44dc0",
            "411a029f6b7343e38735bcdb27443d85",
            "dc9041880dd44451acd9964ca85dbcd8",
            "b747f6627375479d8e755e370db602ad",
            "372447f6ff22495883155129fa51aa21",
            "81f7e8c134b740b9985091b0d7cacd52",
            "6d42ed4f5fd04dba832a027f6c701d3f",
            "a44424b8cc294ae29a481522a40874bd",
            "1aa218e3603f4dd0b6a9d8b2833f646f",
            "a5953f78d15d44b0a873e220d96ed282"
          ]
        },
        "id": "aaUqJrUSNAuq",
        "outputId": "b83a45a5-1b26-4ad9-de43-6fb338343677"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f0f0022b3c14b379d3ae51174e37ad6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = prepare_model_with_mask_predictor(base_model, device)\n",
        "\n",
        "# Apply LoRA for efficient tuning\n",
        "lora_model = apply_lora_to_model(base_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4iqLfjcNZKD",
        "outputId": "3ef176ed-f067-429d-a72e-b4d1d8bc95d4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added mask predictor to CUDA\n",
            "GPU 0: Total 39.56 GB | Reserved 38.99 GB | Allocated 0.02 GB | Free 0.57 GB\n",
            "Applying LoRA for parameter-efficient tuning...\n",
            "trainable params: 20,975,616 || all params: 8,036,560,896 || trainable%: 0.2610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load GSM8K dataset\n",
        "gsm8k_train = load_dataset(\"gsm8k\", \"main\", split=\"train[:100]\")\n",
        "\n",
        "# Create a smaller subset for training\n",
        "SUBSET_SIZE = 2  # Adjust based on your GPU memory\n",
        "gsm8k_dataset = GSM8KDataset(gsm8k_train, tokenizer, subset_size=SUBSET_SIZE)"
      ],
      "metadata": {
        "id": "iqXNJn_SNbNI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "# Setup optimizer groups with different learning rates\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n, p in lora_model.named_parameters()\n",
        "                  if \"lora\" in n and p.requires_grad],\n",
        "        \"lr\": 1e-6,  # Lower learning rate for base model parameters\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n, p in lora_model.named_parameters()\n",
        "                  if \"mask_predictor\" in n and p.requires_grad],\n",
        "        \"lr\": 5e-5,  # Higher learning rate for mask predictor\n",
        "        \"weight_decay\": 0.0,\n",
        "    }\n",
        "]\n",
        "\n",
        "optimizer = Adam(optimizer_grouped_parameters)"
      ],
      "metadata": {
        "id": "np-nS6IONdhB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_model_with_rl(\n",
        "    model=lora_model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=gsm8k_dataset,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=1,\n",
        "    max_steps=32,\n",
        "    accumulation_steps=3,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "\n",
        "lora_model.save_pretrained(\"llada_rl_tuned\")\n",
        "tokenizer.save_pretrained(\"llada_rl_tuned\")"
      ],
      "metadata": {
        "id": "zRKKYSiINfmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.save_pretrained(\"llada_rl_tuned\")\n",
        "tokenizer.save_pretrained(\"llada_rl_tuned\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqyjyKdHTMMo",
        "outputId": "df3b654c-c34d-4f14-bcf7-2190103ecd4d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('llada_rl_tuned/tokenizer_config.json',\n",
              " 'llada_rl_tuned/special_tokens_map.json',\n",
              " 'llada_rl_tuned/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_MODEL = lora_model\n",
        "SAVE_TOK = tokenizer"
      ],
      "metadata": {
        "id": "w_7YOPJgUWrm"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(model, tokenizer, questions, n_shot=4, cot_flag=False,\n",
        "                         max_steps=32, gen_length=64, block_length=8, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generate predictions for a list of questions using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: The fine-tuned LLaDA model\n",
        "        tokenizer: The tokenizer\n",
        "        questions: List of question strings\n",
        "        n_shot: Number of few-shot examples to use\n",
        "        cot_flag: Whether to use chain of thought\n",
        "        max_steps: Maximum diffusion steps\n",
        "        gen_length: Length of text to generate\n",
        "        block_length: Size of diffusion blocks\n",
        "        temperature: Sampling temperature\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing predictions and metadata\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    lora_model.eval()\n",
        "    results = []\n",
        "\n",
        "    for i, question in enumerate(questions):\n",
        "        print(f\"Processing question {i+1}/{len(questions)}\")\n",
        "\n",
        "        # Build prompt with few-shot examples\n",
        "        prompt = build_prompt(question, n_shot, cot_flag)\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        input_ids = inputs.input_ids\n",
        "\n",
        "        # Generate with diffusion\n",
        "        with torch.no_grad():\n",
        "            generated, num_steps = generate_with_step_count(\n",
        "                model,\n",
        "                input_ids,\n",
        "                max_steps=max_steps,\n",
        "                gen_length=gen_length,\n",
        "                block_length=block_length,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            # Decode the generated text\n",
        "            generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract the answer part (after the prompt)\n",
        "            prompt_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "            answer_part = clean_answer(generated_text[len(prompt_text):])\n",
        "\n",
        "            # Extract the numerical answer\n",
        "            numerical_answer = extract_answer_from_output(answer_part)\n",
        "\n",
        "        # Store result\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"full_output\": generated_text,\n",
        "            \"answer_part\": answer_part,\n",
        "            \"numerical_answer\": numerical_answer,\n",
        "            \"steps_used\": num_steps\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Answer: {numerical_answer}, Steps: {num_steps}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Load a fine-tuned model\n",
        "def load_tuned_model(model_path, tokenizer_path, device=None):\n",
        "    \"\"\"Load a fine-tuned model, handling both LoRA and regular models\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "\n",
        "    # Check if this is a LoRA model\n",
        "    is_lora = os.path.exists(os.path.join(model_path, \"adapter_config.json\"))\n",
        "\n",
        "    if is_lora and PEFT_AVAILABLE:\n",
        "        # For LoRA models, load base model first\n",
        "        print(\"Detected LoRA model, loading base model first...\")\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            tokenizer_path,  # Base model path\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "\n",
        "        # Add mask predictor\n",
        "        base_model = prepare_model_with_mask_predictor(base_model, device)\n",
        "\n",
        "        # Load LoRA adapters\n",
        "        print(\"Applying LoRA adapters...\")\n",
        "        from peft import PeftModel\n",
        "        model = PeftModel.from_pretrained(\n",
        "            base_model,\n",
        "            model_path,\n",
        "            torch_dtype=torch.bfloat16\n",
        "        ).to(device)\n",
        "    else:\n",
        "        # Regular saved model\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            low_cpu_mem_usage=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Make sure mask predictor is available\n",
        "        if not hasattr(model.model, 'mask_predictor'):\n",
        "            print(\"Adding mask predictor to loaded model\")\n",
        "            model = prepare_model_with_mask_predictor(model, device)\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "VwKvbEFoNj8d"
      },
      "execution_count": 70,
      "outputs": []
    },
