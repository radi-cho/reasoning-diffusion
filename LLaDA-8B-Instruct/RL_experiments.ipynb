{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f0f0022b3c14b379d3ae51174e37ad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7258984b2ee54f8bbcbb4590b4b44dc0",
              "IPY_MODEL_411a029f6b7343e38735bcdb27443d85",
              "IPY_MODEL_dc9041880dd44451acd9964ca85dbcd8"
            ],
            "layout": "IPY_MODEL_b747f6627375479d8e755e370db602ad"
          }
        },
        "7258984b2ee54f8bbcbb4590b4b44dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_372447f6ff22495883155129fa51aa21",
            "placeholder": "​",
            "style": "IPY_MODEL_81f7e8c134b740b9985091b0d7cacd52",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "411a029f6b7343e38735bcdb27443d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d42ed4f5fd04dba832a027f6c701d3f",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a44424b8cc294ae29a481522a40874bd",
            "value": 6
          }
        },
        "dc9041880dd44451acd9964ca85dbcd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aa218e3603f4dd0b6a9d8b2833f646f",
            "placeholder": "​",
            "style": "IPY_MODEL_a5953f78d15d44b0a873e220d96ed282",
            "value": " 6/6 [00:01&lt;00:00,  3.30it/s]"
          }
        },
        "b747f6627375479d8e755e370db602ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372447f6ff22495883155129fa51aa21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81f7e8c134b740b9985091b0d7cacd52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d42ed4f5fd04dba832a027f6c701d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44424b8cc294ae29a481522a40874bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1aa218e3603f4dd0b6a9d8b2833f646f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5953f78d15d44b0a873e220d96ed282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VErp3AUvJd_E"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from modeling_llada import LLaDAConfig, ModuleType, init_weights\n",
        "from collections import defaultdict\n",
        "\n",
        "rom peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "import subprocess\n",
        "subprocess.check_call([\"pip\", \"install\", \"peft\"])\n",
        "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def print_gpu_memory():\n",
        "    \"\"\"Print current GPU memory usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            total_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
        "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
        "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
        "            free = total_mem - reserved\n",
        "            print(f\"GPU {i}: Total {total_mem:.2f} GB | Reserved {reserved:.2f} GB | Allocated {allocated:.2f} GB | Free {free:.2f} GB\")\n",
        "\n",
        "MAX_LENGTH = 384\n",
        "BATCH_SIZE = 1\n",
        "LEARNING_RATE = 1e-5\n",
        "MASK_HEAD_LR = 5e-5\n",
        "BASE_LR = 1e-6\n",
        "NUM_EPOCHS = 2\n",
        "N_SHOT = 4\n",
        "COT_FLAG = True\n",
        "ANSWER_TRIGGER = \"The answer is\"\n",
        "GEN_LENGTH = 64\n",
        "BLOCK_LENGTH = 8\n",
        "\n",
        "ANS_RE = re.compile(r\"#### (\\-?[0-9\\.\\,]+)\")\n",
        "INVALID_ANS = \"[invalid]\"\n",
        "\n",
        "def extract_answer_from_output(completion):\n",
        "    \"\"\"Extract answer using GSM8K format with #### marker\"\"\"\n",
        "    match = ANS_RE.search(completion)\n",
        "    if match:\n",
        "        match_str = match.group(1).strip()\n",
        "        match_str = match_str.replace(\",\", \"\")\n",
        "        return match_str\n",
        "    else:\n",
        "        return INVALID_ANS\n",
        "\n",
        "def is_correct(model_answer, answer):\n",
        "    \"\"\"Check if model's answer matches ground truth\"\"\"\n",
        "    gt_answer = extract_answer_from_output(answer)\n",
        "    assert gt_answer != INVALID_ANS\n",
        "    return model_answer == gt_answer\n",
        "\n",
        "def clean_answer(model_pred):\n",
        "    \"\"\"Clean and extract numerical answer from model prediction\"\"\"\n",
        "    model_pred = model_pred.lower()\n",
        "    preds = model_pred.split(ANSWER_TRIGGER.lower())\n",
        "    answer_flag = True if len(preds) > 1 else False\n",
        "    if answer_flag:\n",
        "        # Pick first answer with flag\n",
        "        pred = preds[1]\n",
        "    else:\n",
        "        # Pick last number without flag\n",
        "        pred = preds[-1]\n",
        "\n",
        "    pred = pred.replace(\",\", \"\")\n",
        "    pred = [s for s in re.findall(r\"-?\\d+\\.?\\d*\", pred)]\n",
        "\n",
        "    if len(pred) == 0:\n",
        "        return INVALID_ANS\n",
        "\n",
        "    if answer_flag:\n",
        "        # choose the first element in list\n",
        "        pred = pred[0]\n",
        "    else:\n",
        "        # choose the last element in list\n",
        "        pred = pred[-1]\n",
        "\n",
        "    # (For arithmetic tasks) if a word ends with period, it will be omitted\n",
        "    if pred[-1] == \".\":\n",
        "        pred = pred[:-1]\n",
        "\n",
        "    return pred\n",
        "\n",
        "def create_demo_text(n_shot=4, cot_flag=True):\n",
        "    \"\"\"Create demonstration examples for few-shot learning with reduced examples\"\"\"\n",
        "    question, chain, answer = [], [], []\n",
        "    question.append(\n",
        "        \"There are 15 trees in the grove. \"\n",
        "        \"Grove workers will plant trees in the grove today. \"\n",
        "        \"After they are done, there will be 21 trees. \"\n",
        "        \"How many trees did the grove workers plant today?\"\n",
        "    )\n",
        "    chain.append(\n",
        "        \"There are 15 trees originally. \"\n",
        "        \"Then there were 21 trees after some more were planted. \"\n",
        "        \"So there must have been 21 - 15 = 6.\"\n",
        "    )\n",
        "    answer.append(\"6\")\n",
        "\n",
        "    question.append(\n",
        "        \"If there are 3 cars in the parking lot and 2 more cars arrive, \"\n",
        "        \"how many cars are in the parking lot?\"\n",
        "    )\n",
        "    chain.append(\"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.\")\n",
        "    answer.append(\"5\")\n",
        "\n",
        "    question.append(\n",
        "        \"Leah had 32 chocolates and her sister had 42. If they ate 35, \"\n",
        "        \"how many pieces do they have left in total?\"\n",
        "    )\n",
        "    chain.append(\n",
        "        \"Originally, Leah had 32 chocolates. \"\n",
        "        \"Her sister had 42. So in total they had 32 + 42 = 74. \"\n",
        "        \"After eating 35, they had 74 - 35 = 39.\"\n",
        "    )\n",
        "    answer.append(\"39\")\n",
        "\n",
        "    question.append(\n",
        "        \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason \"\n",
        "        \"has 12 lollipops. How many lollipops did Jason give to Denny?\"\n",
        "    )\n",
        "    chain.append(\n",
        "        \"Jason started with 20 lollipops. Then he had 12 after giving some \"\n",
        "        \"to Denny. So he gave Denny 20 - 12 = 8.\"\n",
        "    )\n",
        "    answer.append(\"8\")\n",
        "\n",
        "    index_list = list(range(len(question)))\n",
        "    random.shuffle(index_list)\n",
        "\n",
        "    demo_text = \"\"\n",
        "    for i in index_list[:n_shot]:\n",
        "        if cot_flag:\n",
        "            demo_text += (\n",
        "                \"Q: \"\n",
        "                + question[i]\n",
        "                + \"\\nA: \"\n",
        "                + chain[i]\n",
        "                + \" \"\n",
        "                + ANSWER_TRIGGER\n",
        "                + \" \"\n",
        "                + answer[i]\n",
        "                + \".\\n\\n\"\n",
        "            )\n",
        "        else:\n",
        "            demo_text += (\n",
        "                \"Question: \"\n",
        "                + question[i]\n",
        "                + \"\\nAnswer: \"\n",
        "                + ANSWER_TRIGGER\n",
        "                + \" \"\n",
        "                + answer[i]\n",
        "                + \".\\n\\n\"\n",
        "            )\n",
        "    return demo_text\n",
        "\n",
        "def build_prompt(input_text, n_shot, cot_flag):\n",
        "    demo = create_demo_text(n_shot, cot_flag)\n",
        "    input_text_prompt = demo + \"Q: \" + input_text + \"\\n\" + \"A:\"\n",
        "    return input_text_prompt\n",
        "\n",
        "class GSM8KDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length=MAX_LENGTH, n_shot=N_SHOT, cot_flag=COT_FLAG, subset_size=100):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.n_shot = n_shot\n",
        "        self.cot_flag = cot_flag\n",
        "\n",
        "        self.questions = []\n",
        "        self.answers = []\n",
        "        self.extracted_answers = []\n",
        "\n",
        "        subset = dataset.select(range(min(subset_size, len(dataset))))\n",
        "\n",
        "        for example in subset:\n",
        "            question = example[\"question\"]\n",
        "            answer_with_solution = example[\"answer\"]\n",
        "\n",
        "            numerical_answer = extract_answer_from_output(answer_with_solution)\n",
        "\n",
        "            if numerical_answer != INVALID_ANS:\n",
        "                self.questions.append(question)\n",
        "                self.answers.append(answer_with_solution)\n",
        "                self.extracted_answers.append(numerical_answer)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        answer = self.answers[idx]\n",
        "        extracted_answer = self.extracted_answers[idx]\n",
        "\n",
        "        prompt = build_prompt(question, self.n_shot, self.cot_flag)\n",
        "\n",
        "        tokenized_prompt = self.tokenizer(prompt, return_tensors=\"pt\",\n",
        "                                         padding=\"max_length\",\n",
        "                                         truncation=True,\n",
        "                                         max_length=self.max_length)\n",
        "\n",
        "        ,
        "        return {\n",
        "            \"input_ids\": tokenized_prompt[\"input_ids\"][0],\n",
        "            \"attention_mask\": tokenized_prompt[\"attention_mask\"][0],\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"extracted_answer\": extracted_answer\n",
        "        }\n",
        "\n",
        "# Add Gumbel noise for the diffusion process\n",
        "def add_gumbel_noise(logits, temperature=1.0):\n",
        "    \"\"\"Add Gumbel noise to logits for sampling.\"\"\"\n",
        "    if temperature == 0:\n",
        "        return logits\n",
        "    gumbel = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)\n",
        "    return logits + gumbel * temperature\n",
        "\n",
        "# Memory-efficient generation with step counting\n",
        "def generate_with_step_count(model, prompt, max_steps=32, gen_length=64, block_length=8,\n",
        "                           temperature=0.7, remasking='mask_predictor',\n",
        "                           mask_id=126336, convergence_threshold=0.01, patience=3):\n",
        "    '''Memory-efficient version of the generate function with step counting'''\n",
        "    # Ensure CUDA device\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    if not prompt.is_cuda:\n",
        "        prompt = prompt.to(device)\n",
        "\n",
        "    prompt_length = prompt.shape[1]\n",
        "\n",
        "    x = torch.full((1, prompt_length + gen_length), mask_id, dtype=torch.long, device=device)\n",
        "    x[:, :prompt_length] = prompt.clone()\n",
        "\n",
        "    prompt_index = (x != mask_id)\n",
        "\n",
        "    assert gen_length % block_length == 0\n",
        "    num_blocks = gen_length // block_length\n",
        "\n",
        "    total_steps_used = 0\n",
        "\n",
        "    for num_block in range(num_blocks):\n",
        "        # Clear memory between blocks\n",
        "        if num_block > 0:\n",
        "            clear_memory()\n",
        "\n",
        "        block_start = prompt_length + num_block * block_length\n",
        "        block_end = prompt_length + (num_block + 1) * block_length\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            mask_index = (x == mask_id)\n",
        "\n",
        "            current_block_mask_indices = mask_index[:, block_start:block_end]\n",
        "            num_masked = current_block_mask_indices.sum().item()\n",
        "\n",
        "            if num_masked == 0:\n",
        "                break\n",
        "\n",
        "            total_steps_used += 1\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(x)\n",
        "                logits = outputs.logits\n",
        "                mask_logits = outputs.mask_logits if hasattr(outputs, 'mask_logits') else None\n",
        "\n",
        "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
        "            x0 = torch.argmax(logits_with_noise, dim=-1)  # b, l\n",
        "\n",
        "            if remasking == 'mask_predictor' and mask_logits is not None:\n",
        "                # Use mask predictor scores directly\n",
        "                confidence = torch.sigmoid(mask_logits.squeeze(-1))  # Convert to probabilities\n",
        "            elif remasking == 'low_confidence':\n",
        "                p = F.softmax(logits, dim=-1)\n",
        "                x0_p = torch.squeeze(\n",
        "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1)\n",
        "                confidence = 1.0 - x0_p  # Invert so higher means less confident\n",
        "            elif remasking == 'random':\n",
        "                confidence = torch.rand((x0.shape[0], x0.shape[1]), device=device)\n",
        "            else:\n",
        "                raise NotImplementedError(remasking)\n",
        "\n",
        "            confidence[:, :block_start] = float('-inf')  # Before current block\n",
        "            confidence[:, block_end:] = float('-inf')    # After current block\n",
        "\n",
        "            x0 = torch.where(mask_index, x0, x)\n",
        "\n",
        "            remaining_ratio = 1.0 - (step / max_steps)\n",
        "            num_to_unmask = max(1, int(num_masked * remaining_ratio * 0.5))  # Unmask fewer tokens at once\n",
        "\n",
        "            remask_candidates = mask_index.clone()\n",
        "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=device)\n",
        "\n",
        "            for j in range(confidence.shape[0]):\n",
        "                masked_confidence = confidence[j].clone()\n",
        "                masked_confidence[~remask_candidates[j]] = float('-inf')\n",
        "\n",
        "                _, select_index = torch.topk(masked_confidence,\n",
        "                                          k=min(num_to_unmask, torch.sum(remask_candidates[j]).item()),\n",
        "                                          largest=False)\n",
        "                transfer_index[j, select_index] = True\n",
        "\n",
        "            x[transfer_index] = x0[transfer_index]\n",
        "\n",
        "            del logits, mask_logits, x0, confidence, masked_confidence\n",
        "            clear_memory()\n",
        "\n",
        "            if not (x[:, block_start:block_end] == mask_id).any():\n",
        "                break\n",
        "\n",
        "        remaining_mask_indices = (x[:, block_start:block_end] == mask_id)\n",
        "        if remaining_mask_indices.any():\n",
        "            total_steps_used += 1\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(x)\n",
        "                logits = outputs.logits\n",
        "                logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
        "                x0 = torch.argmax(logits_with_noise, dim=-1)\n",
        "\n",
        "            mask_indices_full = (x == mask_id)\n",
        "            block_mask_indices = torch.zeros_like(mask_indices_full, device=device)\n",
        "            block_mask_indices[:, block_start:block_end] = remaining_mask_indices\n",
        "\n",
        "            x = torch.where(block_mask_indices, x0, x)\n",
        "\n",
        "            del logits, x0, mask_indices_full, block_mask_indices\n",
        "            clear_memory()\n",
        "\n",
        "    return x, total_steps_used\n",
        "\n",
        "# Custom reward calculation\n",
        "def calculate_reward(generated_text, target_answer, num_steps_used, max_steps):\n",
        "    \"\"\"\n",
        "    Calculate reward based on correctness and efficiency of diffusion.\n",
        "\n",
        "    Args:\n",
        "        generated_text: Text generated by the model\n",
        "        target_answer: The correct numerical answer\n",
        "        num_steps_used: Number of diffusion steps used\n",
        "        max_steps: Maximum allowed diffusion steps\n",
        "\n",
        "    Returns:\n",
        "        float: Calculated reward\n",
        "    \"\"\"\n",
        "    predicted_answer = extract_answer_from_output(generated_text)\n",
        "\n",
        "    if predicted_answer == INVALID_ANS:\n",
        "        return -5.0  # Penalty for no clear answer\n",
        "\n",
        "    correctness_reward = 10.0 if predicted_answer == target_answer else -5.0\n",
        "\n",
        "    efficiency_factor = max(0, 2.0 - 2.0 * (num_steps_used / max_steps))\n",
        "    efficiency_reward = 5.0 * efficiency_factor\n",
        "\n",
        "    total_reward = correctness_reward + efficiency_reward\n",
        "\n",
        "    return total_reward\n",
        "\n"
        "class UpdatedLLaDAOutput:\n",
        "    def __init__(self, logits, attn_key_values=None, hidden_states=None, mask_logits=None):\n",
        "        self.logits = logits\n",
        "        self.attn_key_values = attn_key_values\n",
        "        self.hidden_states = hidden_states\n",
        "        self.mask_logits = mask_logits\n",
        "\n",
        "def prepare_model_with_mask_predictor(base_model, device):\n",
        "    \"\"\"Add mask predictor to model and patch forward methods to handle mask prediction\"\"\"\n",
        "\n",
        "    d_model = base_model.config.d_model\n",
        "    include_bias = base_model.config.include_bias\n",
        "\n",
        "    base_model.model.mask_predictor = torch.nn.Linear(\n",
        "        d_model,\n",
        "        1,\n",
        "        bias=include_bias,\n",
        "    ).to(device).to(torch.bfloat16)\n",
        "\n",
        "\n",
        "    init_weights(\n",
        "        base_model.model.config,\n",
        "        base_model.model.mask_predictor,\n",
        "        d=d_model,\n",
        "        type_of_module=ModuleType.out_module\n",
        "    )\n",
        "\n",
        "    original_forward = base_model.model.forward\n",
        "\n",
        "    def forward_with_mask_logits(*args, **kwargs):\n",
        "        outputs = original_forward(*args, **kwargs)\n",
        "\n",
        "        hidden_states = outputs.hidden_states[-1]\n",
        "   
        "\n",
        "        mask_logits = None\n",
        "        if hidden_states is not None:\n",
        "            mask_logits = base_model.model.mask_predictor(hidden_states)\n",
        "\n",
        "        return UpdatedLLaDAOutput(\n",
        "            logits=outputs.logits,\n",
        "            attn_key_values=getattr(outputs, 'attn_key_values', None),\n",
        "            hidden_states=getattr(outputs, 'hidden_states', None),\n",
        "            mask_logits=mask_logits\n",
        "        )\n",
        "\n",
        "    base_model.model.forward = forward_with_mask_logits\n",
        "\n",
        "    original_hf_forward = base_model.forward\n",
        "\n",
        "    def hf_forward_with_mask_logits(*args, **kwargs):\n",
        "        kwargs['output_hidden_states'] = True\n",
        "\n",
        "        outputs = original_hf_forward(*args, **kwargs)\n",
        "\n",
        "        if not hasattr(outputs, 'mask_logits'):\n",
        "            if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
        "                last_hidden_state = outputs.hidden_states[-1]\n",
        "                setattr(outputs, 'mask_logits', base_model.model.mask_predictor(last_hidden_state))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    base_model.forward = hf_forward_with_mask_logits\n",
        "\n",
        "    return base_model\n",
        "\n",
        "def train_full_model_with_rl(model, tokenizer, dataset, optimizer, num_epochs=2, max_steps=32,\n",
        "                         accumulation_steps=4, fp16=True):\n",
        "    
        "    device = torch.device(\"cuda:0\")\n",
        "    scaler = torch.cuda.amp.GradScaler() if fp16 else None\n",
        "\n",
        "    model.train()\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    all_rewards = []\n",
        "    all_steps = []\n",
        "    correct_answers = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    
